{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2625a3c3-2bac-4637-969c-309137650ce4",
   "metadata": {},
   "source": [
    "---\n",
    "title: STATS/CSE 780 - Project Report\n",
    "author: \"Xiaoran Xie (Student number: 400549859)\"\n",
    "date: \"2023/12/11\"\n",
    "format: pdf\n",
    "execute: \n",
    "  echo: true\n",
    "  message: false\n",
    "  warning: false\n",
    "bibliography: STATS780.bib\n",
    "fontsize: 11pt\n",
    "geometry: \n",
    "  - margin = 1in\n",
    "linestretch: 1.5\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862186aa-c281-4a8a-86bb-321251edfa8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\\newpage\n",
    "## Abstract\n",
    "\n",
    "Stroke, as an epidemic that kills millions of people annually, representing a significant challenge in healthcare, has become a health issue that we need to be concerned about. Predicting stroke risk is important since early detection and intervention are key to reducing its prevalence. If the risk of stroke can be predicted based on an individual’s health information, it will potentially save lives and reduce the impact on non-patients, as well as help patients to receive timely treatment and reduce mortality[@data2]. Acccording to data preprocessing and data transformation, we cleaned the data by handling missing value and outlier, converted catagorical data to numerical data,  and so on. By exploratory data analysis, we found the correlated factors between various predictors and the occurrence of stroke. We identified key factors influencing stroke risk and dived into the analysis by looking for the relationship. For stroke diagnosis prediction and classification, we trained two different machine learning models to do analysis and comparison. Mutilayer Perceptron has better accurancy, but whether it is a good matched model for our project to predict the stroke is still under consideration. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This dataset[@data] contains health information on 5110 patients with confirmed or undiagnosed stroke obtained from the Kaggle platform, which is detailed and helps to find potential risk factors. The main goal of the project is to find the factors most associated with stroke risk and to build a prediction model based on this information. By developing a predictive model based on these factors, we can provide doctors with a powerful tool for assessing stroke risk more accurately. This will help doctors more accurately assess a patient’s stroke risk and give timely medical advice. In this project, we will use two supervised learning methods in machine learning to build risk prediction models that enable classification of stroke diagnoses.\n",
    "\n",
    "## Methods\n",
    "\n",
    "In developing our stroke risk prediction model, we chose a combination of Random Forest and Multilayer Perceptron for classification. Random forest is an ensemble model composed of multiple decision trees. It can provide the features that have the greatest impact on the prediction results and can effectively handle non-linear relationships, which is very useful for understanding the data. Although it is different from the high transparency and interpretability of decision trees, compared with multilayer perceptrons, it can still be tracked and understood through the information of tree nodes. Deep learning models, especially multi-layer perceptrons, are good at learning and representing complex non-linear relationships through the stacking of multiple hidden layers. And deep learning can perform well on large-scale data sets. However, there are many hyperparameters, such as the number of layers, number of neurons, learning rate, etc. Therefore, parameter adjustment and training are costly and require a large amount of computing resources, which will indirectly affect the progress of the experiment. Furthermore, the model often has lower interpretability compared to other traditional machine learning methods.\n",
    "\n",
    "During Random Forest parameter tuning, we explored different splitting criteria (functions for measuring the quality of data splits), max_depth, and n_estimators (the number of trees in the forest). The choice of gini, entropy, and log_loss as splitting criteria was to assess the impact of different data impurity measurements on model performance. The choice of max_depth aimed to find the optimal depth that prevents overfitting while allowing the model to learn sufficient data. The choice of n_estimators was to determine the required number of trees while maintaining computational efficiency. Ultimately, the optimal parameter combination found was criterion: 'gini', max_depth: None, and n_estimators: 100, indicating that not limiting the depth of the decision trees and using 100 trees could achieve the best cross-validation score.\n",
    "\n",
    "During Multilayer Perceptron parameter tuning, we considered different hidden_layer_sizes, activations (activation functions), solvers (optimization algorithms), learning_rate, and max_iter (maximum number of iterations). The choice of activation functions, such as identity, logistic, tanh, and relu define how neurons process input signals. The choice of lbfgs, sgd, and adam as solvers was to compare the efficiency and convergence of different optimization algorithms. The optimal parameters were activation: 'tanh', hidden_layer_sizes: 32, and learning_rate: 'invscaling', indicating that on the given dataset, using the hyperbolic tangent activation function, a single hidden layer with 32 neurons, and a learning rate that decreases with the number of iterations, could achieve the optimal cross-validation score.\n",
    "\n",
    "## Results\n",
    "\n",
    "For handling missing BMI values in the dataset, we constructed a prediction model that exploited the correlation between the age and gender variables and BMI to estimate missing values from the relevant variables. Specifically, we implemented a pipeline[@data3] containing two main processing steps: standardization of the data, followed by the application of a decision tree regression model. With this approach, we ensure consistency of model inputs. The standardization process ensures that each feature has the same weighting during model training, while the decision tree regressor provides us with a nonlinear way to predict missing values. We first excluded subsets containing missing BMI records. Then, the remaining complete data was utilized to train the pipeline model. We used the model to make predictions about missing BMI values and filled these predictions back into the original dataset at the appropriate locations. This method not only enhances the completeness of the data, but also improves the reliability of the subsequent analysis steps. We confirmed the validity of the method with updated missing value statistics.\n",
    "\n",
    "Then we compared two encoding ways to transform categorical variables. The first method we used is one-hot encoding, a technique that creates binary columns for each category of a variable, which indicate the presence of a feature with a value of 1, while absence is marked with a value of 0. One-hot encoding does not impose any arbitrary ordering on the categories (e.g., 'gender-Male' is not greater than 'gender-Female'). This approach is beneficial when the categorical variable does not have an intrinsic order or ranking, and it ensures that the model does not attribute importance to the categories based on their encoded numerical values. The second method is label encoding, like enumeration, which assigns a unique integer to each category. Eventhough this method is more efficient and simpler, it assumes an ordinal relationship between the categories. So it could be misleading if applied to data where no such ordinal relationship exists. Combined with our data set, one-hot encoding better matches, and it proves to be more effective for our analysis than label encoding.\n",
    "\n",
    "After we finished data type transformation, the original dataset was updated and suitbale to do exploratory data analysis. \n",
    "\n",
    "The below bar chart visualizes the correlation coefficients between various predictors and the occurrence of stroke. Age shows the strongest positive correlation, indicating a higher stroke risk with increasing age. Heart disease and higher average glucose levels also show a moderate positive correlation, which means they are significant risk factors. Hypertension, with a similar correlation coefficient, which is consistent with known risk to stroke. Marital status unexpectedly shows to have a positive correlation, we didn't know whether there are some protential social or psychological influences on stroke risk. Smoking history is also positively correlated on cardiovascular health. \n",
    "\n",
    "![tg_c](Target_Correlation.png)\n",
    "This below heatmap shows a comprehensive factor correlation, which represents the correlation coefficients between various variables in the dataset. The color intensity in the heatmap reflects the strength and direction of the correlation, with darker colors indicating stronger relationships. This figure is critical for identifying potential predictors of stroke. It can aid feature selection for predictive modeling by highlighting which variables are most closely associated with the occurrence of stroke. From the figure, it's clear that age has a notable positive correlation with the occurrence of stroke. This suggests that as people age, the risk of stroke increases. The average glucose level also shows a positive correlation with stroke, which is consistent with medical knowledge that elevated glucose levels can lead to vascular complications contributing to stroke risk. Additionally, the relationship between average glucose level and BMI is crucial, as it can indicate how body weight and glucose metabolism may jointly impact the risk of stroke. \n",
    "\n",
    "![fc_c](Factor_Correlation.png)\n",
    "Thus, we focused on the factors which are average glucose level, age and BMI. There are two figures below seem to describle two scatter plots with regression lines, one showing the relationship between average glucose level and age, and the other between average glucose level and BMI, each colored by the stroke outcome.\n",
    "\n",
    "![avg_age](avg_age.png)\n",
    "![avg_bmi](avg_bmi.png)\n",
    "In the first figure about average glucose level and age, we could observe that as the average glucose level increases, the age of individuals also increases, which is indicated by the upward trend of the regression line. The color differentiation likely shows that higher average glucose levels and older age may correlate with a higher incidence of stroke, as indicated by the presence of more data points for stroke occurrences in these ranges. The second figure shows the relationship between average glucose levels and BMI. However, the regression line is less steep and almost close to flat, which might present a weaker relationship between glucose levels and BMI than with age. If we see clusters of stroke occurrences at higher glucose levels regardless of BMI, it could indicate that glucose levels are a more critical risk factor for stroke than BMI. These data visualizations could bring us revealing outcomes. If both age and average glucose levels show a clear positive trend with stroke occurrence but BMI does not, we would consider that the timely treatment to prevent stroke should prioritize glucose control and monitoring in older people instead of BMI.\n",
    "\n",
    "So far, we had implemented a series exploratory data analysis, which give us insights on the critical features of prredicting stroke risk. Since our main goal is to build a model that is both accurate and clinically relevant, providing healthcare professionals with a reliable tool for assessing stroke risk, we started to the feature scaling and model selection. Firstly, the dataset is split into two parts, around 75% for training to build the model and 25% for testing its predictions. This split is essential to validate the model's performance on unseen data. A significant challenge addressed next is the sample imbalance, which is the prevalent in medical datasets. We had only 4.9% for dignosised stroke smaples and 95.1% is taken up by unconfirmed stroke samples. To solve this issue to reduce the bias in the model towards the majority class, we used the Synthetic Minority Over-sampling Technique (SMOTE), which synthetically augments the minority class by generating new examples. This technique ensures that our predictive model is exposed to a balanced view of the stroke occurrences, mitigating the risk of bias. Then, we implemented data normalization to scale the feature values and ensure that the range of our data values didn't influence the model's performance. This step is particularly important as it ensures that each feature contributes proportionately to the model's learning process, thereby enhancing the model's ability to learn patterns effectively and improving algorithm convergence during training. These preprocessing steps build a robust foundation for developing our stroke prediction model, so that the data is becoming representative, balanced, and scaled appropriately for the algorithm.\n",
    "\n",
    "The evaluation of our models was thorough, utilizing both the confusion matrix and the ROC curve. The confusion matrix helped us understand the accuracy of our classifications, which reveals the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions. And the ROC curve is instrumental in assessing the performance of a binary classifier system, which provides insights into the balance between sensitivity and specificity. It plots the true positive rate against the false positive rate at various threshold settings. And the area under the curve (AUC) is a measure of the model's ability to distinguish between the two classes. \n",
    "\n",
    "The first model we selected is Random Forest, after cross-validation we found optimal hyperparameters and utilized them to train and test the model. The below left figure is Random Forest model's confusion matrix. We see a high number of TN predictions, where the model correctly predicts the non-occurrence of strokes, and a relatively low number of TP predictions, indicating the model's challenge in correctly identifying actual stroke cases. This discrepancy is a common issue in medical prediction models, especially when dealing with imbalanced datasets where the occurrence of one class (stroke) is much less frequent than the other (non-stroke). The below right figure is Random Forest model's ROC curve. An AUC of 0.78 suggests that the model has a reasonable distinction capacity, although there is room for improvement, especially in the context of medical predictions where higher accuracy is necessary. In summary, the Random Forest model shows promising results but also reflects the challenge of predicting relatively rare cases such as strokes. The use of techniques like SMOTE for oversampling and rigorous hyperparameter tuning through grid search is evidenced to enhance the model's performance. However, the model's difficulty in accurately identifying stroke cases, we could look for further refinement, through advanced feature engineering, sample scaling strategies, or different algorithms to improve the predictive accuracy for the minority class. This level of detail in analysis ensures that the model's predictive power aligns closely with the critical healthcare objective of accurately identifying stroke risk.\n",
    "\n",
    "![Random_Forest_Model_cm](Random_Forest_Model_cm.png){width=50%}\n",
    "![Random_Forest_Model_roc](Random_Forest_Model_roc.png){width=50%}\n",
    "The second model we selected is Mutilayer Perception, after cross-validation we found optimal hyperparameters and utilized them to train and test the model. The below left figure is Mutilayer Perception model's confusion matrix. Simmilar to the Random Forest model results, Mutilayer Perception model also has a high number of TN predictions, which means it generates many false positives and failed to catch numerous stroke cases, leading to a lower recall for these critical cases and therefore a greater chance of overlooking actual stroke patients. Eventhough it ranks higher in accuracy and AUC, but it couldn't detect stroke cases. This is a major drawback for medical diagnosis where missing a true case can have serious consequences. From these results, we would say that neither model is effective for stroke prediction, but Mutilayer Perception could be a choice if there is no other models to compare.\n",
    "\n",
    "![Multilayer_Perception_Model_cm](Multilayer_Perception_Model_cm.png){width=50%}\n",
    "![Multilayer_Perception_Model_roc](Multilayer_Perception_Model_roc.png){width=50%}\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The main goal of the project is to identify key factors influencing stroke risk. By developing a predictive model based on these factors, we can provide doctors with a powerful tool for assessing stroke risk more accurately. This not only aids in medical professionals in risk assessment, but also offers patients timely medical advice, providing actionable insights. \n",
    "\n",
    "According to the exploratory data analysis summaries, we found the correlated factors between various features and occurrence of stroke. We identified key factors influencing stroke risk such as numerical variables age, average glucose level and BMI. We made a lot of relevant speculations and thoughts based on the data and visualization results. For instance, higher average glucose levels and older age may correlate with a higher incidence of stroke. These analysises can help us to determine whether features are useful. \n",
    "\n",
    "For stroke dignosis classifier, we compared two classification model Random Forest and Multilayer Perception. From the results they got, the Random Forest classifier with an overall accuracy of 91.85%, while the Multilayer Perceptron classifier indicates a higher overall accuracy of 94.63%. The second model performs slightly better in terms of overall accuracy. As we mentioned before, both models perform well on the majority class, with high precision, recall, and F1-scores. This is expected due to the class imbalance. This is a challenge for us when using machine learning and the algorithm can lead to biased models that are better at predicting the majority class at the expense of the minority class.\n",
    "Altogether, the Multilayer Perceptron model has a better balance between precision and recall for the minority class, as indicated by the higher precision. Therefore, for this case to predict stroke risk, using Multilayer Perceptron would be better than Random Forest, eventhough both of them have low recall for class 1 (stroke), indicating many false negatives (non-stroke).\n",
    "\n",
    "Although our current results do not meet our expectations, we believe that both models would benefit from techniques to handle the class imbalance, such as oversampling the minority class or using anomaly detection methods, we will try different techniques in the future. This is what we would continue to try next.\n",
    "\n",
    "\\newpage\n",
    "\n",
    "## Supplementary materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e444e7-825b-4c42-844a-3a3c5b211dac",
   "metadata": {},
   "source": [
    "| Feature           | Description                                                  |\n",
    "|-------------------|--------------------------------------------------------------|\n",
    "| id                | unique identifier                                            |\n",
    "| gender            | \"Male\", \"Female\" or \"Other\"                                  |\n",
    "| age               | age of the patient                                           |\n",
    "| hypertension      | 0 if the patient doesn't have hypertension, 1 if they do     |\n",
    "| heart_disease     | 0 if the patient doesn't have any heart diseases, 1 if they do |\n",
    "| ever_married      | \"No\" or \"Yes\"                                                |\n",
    "| work_type         | \"children\", \"Govt_job\", \"Never_worked\", \"Private\" or \"Self-employed\" |\n",
    "| Residence_type    | \"Rural\" or \"Urban\"                                           |\n",
    "| avg_glucose_level | average glucose level in blood                               |\n",
    "| bmi               | body mass index                                              |\n",
    "| smoking_status    | \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"     |\n",
    "| stroke            | 1 if the patient had a stroke or 0 if not                    |\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "314edd75-3c70-40d5-8c1d-0e1d6d6ef083",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from graphviz import Source\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4c11c94-e8eb-4d31-9684-e2df7e55cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "#print(df.shape)\n",
    "#print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ee7cd7-240b-41c0-b6ac-7630919a096e",
   "metadata": {},
   "source": [
    "### Handle Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ac1dd-3d4b-4b75-a523-1e85749ec3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b199b99-0ba3-4eef-a0d0-6df131032641",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_bmi_pipe = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('lr', DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "temp_X = df[['age','gender','bmi']].copy()\n",
    "temp_X.gender = temp_X.gender.replace({\n",
    "    'Male':0,'Female':1,'Other':-1\n",
    "}).astype(np.uint8)\n",
    "missing = temp_X[temp_X.bmi.isna()]\n",
    "temp_X = temp_X[~temp_X.bmi.isna()]\n",
    "temp_Y = temp_X.pop('bmi')\n",
    "dt_bmi_pipe.fit(temp_X,temp_Y)\n",
    "predicted_bmi = pd.Series(dt_bmi_pipe.predict(\n",
    "    missing[['age','gender']]),index=missing.index)\n",
    "df.loc[missing.index,'bmi'] = predicted_bmi\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec4b815-24d4-4529-9b7e-d66aba144d65",
   "metadata": {},
   "source": [
    "### Encode Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "911ef5dd-142f-4999-8142-70ade4e9fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['age', 'bmi', 'avg_glucose_level']\n",
    "categorical_columns = ['gender', 'hypertension', 'heart_disease',\n",
    "                       'ever_married', 'work_type', 'Residence_type',\n",
    "                       'smoking_status', 'stroke']\n",
    "# use dictionary to save column name as key, save categories as value\n",
    "object_categorical_columns_dict = {} \n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        object_categorical_columns_dict[col]=df[col].value_counts().index.to_list()          \n",
    "#print(object_categorical_columns_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bfe1cb-f09c-496c-8bc0-707ca2df5e76",
   "metadata": {},
   "source": [
    "#### Method 1: One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1987e56b-f889-4f28-a949-e8baf2153122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f089b48c-a021-400f-bccb-cfc2ad4a3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for multiple categories\n",
    "df = pd.get_dummies(df, columns=object_categorical_columns_dict.keys())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d067952-d209-4ed5-8a45-b6dc6670d91c",
   "metadata": {},
   "source": [
    "#### Method 2: Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f4ab9-bc74-4ebd-adab-ccf5b562dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_values = {}\n",
    "temp_dict = {}\n",
    "print('_'*45)\n",
    "for col, values in object_categorical_columns_dict.items():\n",
    "    print(f'column: {col}')\n",
    "    for index, val in enumerate(values):\n",
    "        temp_dict[val] = index\n",
    "        print(f'{val}: {index}')\n",
    "    encoded_values[col] = temp_dict\n",
    "    df[col] = df[col].replace(temp_dict)\n",
    "    print('_'*45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f6269-6ff6-4435-bdda-ec8aae6d412d",
   "metadata": {},
   "source": [
    "### Explore and Visualise Data\n",
    "stroke: 1 if the patient had a stroke or 0 if not[@data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051eb7b9-594f-4029-b9ed-71c1b4144726",
   "metadata": {},
   "source": [
    "#### Numeric Columns: Age, bmi, avg_glucose_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a5f59117-7577-413b-a7d7-551fa380255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "fig, ax = plt.subplots(3, 3, figsize=(18, 8))\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "for num_col in numeric_columns :\n",
    "    sns.kdeplot(x=num_col, hue='stroke', data=df, multiple='stack', ax=ax[i,0])\n",
    "    sns.boxplot(x=num_col, data=df, ax=ax[i, 1])\n",
    "    sns.scatterplot(x=num_col, y='stroke', data=df, ax=ax[i, 2])\n",
    "    i+=1\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae961d-df11-4798-9709-52b4429a2eb1",
   "metadata": {},
   "source": [
    "Those who have had a stroke are in: age in range 40 to 85, bmi in range 20 to 40, glocuse level in range 50 to 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "739f254b-6f39-40eb-b915-a2b44a8aa790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation for the target variable (stroke)\n",
    "plt.figure(figsize = (15,5))\n",
    "sns.heatmap(df.corr()[['stroke']].sort_values(\n",
    "    by='stroke', ascending=False), annot = True, cmap = 'Blues')\n",
    "plt.title('Target Correlation',fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'Target_Correlation.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24643440-422a-4a7e-ac78-aaf448e07c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between each factor\n",
    "corr = df.corr()\n",
    "plt.figure(figsize = (18,10))\n",
    "sns.heatmap(corr, cmap = 'Blues', annot = True)\n",
    "plt.title(\"Factor Correlation\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'Factor_Correlation.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e7c8bea-3db5-4d86-9690-33c8429c4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "sns.scatterplot(data=df, x='avg_glucose_level',y='age',hue='stroke')\n",
    "sns.regplot(data=df, x='avg_glucose_level', y='age', \n",
    "            scatter=False, ci=95, line_kws={\"color\": \"blue\"})\n",
    "plt.xlabel('avg_glucose_level')\n",
    "plt.ylabel('age')\n",
    "plt.title('avg_glucose_level & age')\n",
    "plt.legend(labels=['1: Stroked', '0: Non Stroked'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'avg_age.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1bd6ea4c-21c2-4bef-91d4-7eafa3c29d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "sns.scatterplot(data=df, x='avg_glucose_level',y='bmi',hue='stroke')\n",
    "sns.regplot(data=df, x='avg_glucose_level', y='bmi', \n",
    "            scatter=False, ci=95, line_kws={\"color\": \"blue\"})\n",
    "plt.xlabel('avg_glucose_level')\n",
    "plt.ylabel('bmi')\n",
    "plt.title('avg_glucose_level & bmi')\n",
    "plt.legend(labels=['1: Stroked', '0: Non Stroked'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'avg_bmi.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2dd1c5-c2f4-4869-99ae-5ad34f3751be",
   "metadata": {},
   "source": [
    "## Feature Scaling and Upscaling\n",
    "### Data Splitting\n",
    "split the data into training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b94bc1d3-b7ea-49bd-abc9-443e9575985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='stroke')\n",
    "y = df['stroke']\n",
    "X.columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521682e7-9608-4740-95f9-19c0bc099139",
   "metadata": {},
   "source": [
    "### Handle Data Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aebb16b1-7395-421e-bb6d-0727099354b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['stroke'].value_counts()\n",
    "explode = [0, 0.1]\n",
    "fig, ax = plt.subplots(figsize=(4, 4),subplot_kw=dict(aspect=\"equal\"))\n",
    "plt.pie(x,explode=explode,autopct='%1.1f%%',textprops=dict(color=\"w\",size=10))\n",
    "plt.legend(labels = ['0: Non Stroke', '1: Stroke'])\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0af02703-8dc6-4e85-b3b2-90f5f7793e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({0: 3663, 1: 169})\n",
      "Oversampled class distribution: Counter({0: 3663, 1: 3663})\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "print(f'Original class distribution: {Counter(y_train)}')\n",
    "print(f'Oversampled class distribution: {Counter(y_train_oversampled)}')\n",
    "X_train = X_train_oversampled\n",
    "y_train = y_train_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d3035-2d0c-4219-9bad-2abc50aa93d9",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4eefb5e8-eb51-4dd0-bac4-650aa1490488",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "#X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c41e2-10f7-4069-8a4b-0a21e97d8c5e",
   "metadata": {},
   "source": [
    "## Data Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f7722db-4032-4d15-beff-f7e945327300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm(y_test, y_predict, modelName):\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title(f'Confusion Matrix for {modelName}')\n",
    "    plt.savefig(f'{modelName}_cm.png')\n",
    "    plt.close()\n",
    "    \n",
    "def roc(y_test, y_pred_probs, modelName):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC for {modelName}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'{modelName}_roc.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8eb663-481e-467b-b925-e673e921144c",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf12ce-b003-4c3e-9ddd-440394661d3d",
   "metadata": {},
   "source": [
    "### Model 1: Random Forest\n",
    "#### Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8939e28-e8c7-4f2d-8597-89c41165e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paremeter grid, create cross validation by grid search\n",
    "param_gs = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [3,5, None],\n",
    "    'n_estimators': [50, 100]\n",
    "}\n",
    "cv_rf = RandomForestClassifier()\n",
    "cv_gs_rf = GridSearchCV(estimator=cv_rf, cv=5, param_grid=param_gs)\n",
    "cv_gs_rf.fit(X_train, y_train)\n",
    "print(\"Optimal tuning parameters: \", cv_gs_rf.best_params_)\n",
    "print(\"Optimal cross-validation score: \", cv_gs_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f3d98-13eb-4573-9d25-ad8b0248a5a9",
   "metadata": {},
   "source": [
    "#### Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783d2d9-cc42-40bb-adf1-e286c0034697",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = cv_gs_rf.best_params_['criterion']\n",
    "max_depth = cv_gs_rf.best_params_['max_depth']\n",
    "n_estimators = cv_gs_rf.best_params_['n_estimators']\n",
    "rf = RandomForestClassifier(\n",
    "    criterion=criterion, max_depth=max_depth, n_estimators=n_estimators)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_score = round(rf.score(X_test, y_test), 5)\n",
    "y_predict = rf.predict(X_test)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(f'Accuracy: {rf_score}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00385cea-4b75-4736-ad36-400a9c979be0",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffca91b-a0a8-415d-847b-9de3e02bdf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'Random_Forest_Model'\n",
    "cm(y_test, y_predict, modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a70f2e-b027-4e38-98e4-0ce09a773775",
   "metadata": {},
   "source": [
    "#### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab132b2c-639d-43d3-91ec-dd954fc5d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = rf.predict_proba(X_test)[:, 1]\n",
    "roc(y_test, y_pred_prob, modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a543a8-fe09-4747-a17d-a0150ae93e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier()\n",
    "random_forest_model.fit(X, y)\n",
    "dt_fit = random_forest_model.estimators_[0]\n",
    "dot_data = export_graphviz(dt_fit, out_file=None, feature_names=X.columns,\n",
    "                           class_names=['0: Non Stroke', '1: Stroke'],\n",
    "                           filled=True, rounded=True, special_characters=True)\n",
    "graph = Source(dot_data)\n",
    "graph.render(f\"{modelName}_graph\", format='png', cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9691c4-e8a7-44be-969b-4da22a8eab25",
   "metadata": {},
   "source": [
    "### Model 2: Multilayer Perception\n",
    "#### Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f803c7-fe5e-4eb9-9be3-5615d73a8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gs = {\n",
    "    'hidden_layer_sizes':[8,16,32],\n",
    "    'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'learning_rate':['constant', 'invscaling', 'adaptive'],\n",
    "    'max_iter': [100,200],'warm_start':[True]\n",
    "}\n",
    "cv_mlp = MLPClassifier()\n",
    "cv_gs_mlp = GridSearchCV(estimator=cv_mlp, param_grid=params_gs,\n",
    "                         cv=5, scoring='accuracy')\n",
    "cv_gs_mlp.fit(X_train, y_train)\n",
    "print(\"Optimal tuning parameters: \", cv_gs_mlp.best_params_)\n",
    "print(\"Optimal cross-validation score: \", cv_gs_mlp.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27333f68-96b4-44b7-9fc1-858fb41693f6",
   "metadata": {},
   "source": [
    "#### Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7853d-6774-4eb0-afdd-0e9c4105f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(**cv_gs_mlp.best_params_)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_predict = mlp.predict(X_test)\n",
    "report = classification_report(y_test, y_predict)\n",
    "mlp_score = mlp.score(X_test, y_test)\n",
    "print(f'Accuracy: {mlp_score}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083e644-3aca-4365-81c1-7af1006bf87c",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3dc61-f94c-4140-b0dd-bd0ee8cda833",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'Multilayer_Perception_Model'\n",
    "cm(y_test, y_predict, modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58104903-37d5-4564-8e5c-a28b3a94f22b",
   "metadata": {},
   "source": [
    "#### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea57c9-29a7-4d21-b237-5506137727a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = mlp.predict_proba(X_test)[:, 1]\n",
    "roc(y_test, y_pred_prob, modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0486d-2306-44f8-a920-ea7669ebcc0f",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fcc89-a568-4530-a6c1-4c911fb788bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
